{"cells":[{"cell_type":"markdown","metadata":{"id":"D4u5Ao_Z1fBt"},"source":["# Cell 1: Install required dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24232,"status":"ok","timestamp":1745683385338,"user":{"displayName":"Firass Aguech","userId":"16185286434426551254"},"user_tz":-60},"id":"vja4JEh71iCt","outputId":"37541bf4-d8b5-477a-8a32-437187c88893"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"LZAmHJc91fBu"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/471.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m122.9/471.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m471.0/471.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch torchvision accelerate transformers\n","!pip install -q diffusers peft bitsandbytes\n","!pip install -q pillow ftfy gradio\n","!pip install -q safetensors wandb pyyaml"]},{"cell_type":"markdown","metadata":{"id":"SfPHQqJO1fBv"},"source":["# Cell 2: Import necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MikZSHR01fBv"},"outputs":[],"source":["import os\n","import yaml\n","import math\n","import random\n","import logging\n","import argparse\n","from pathlib import Path\n","from typing import Dict, List, Union\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.utils.data import Dataset, DataLoader\n","\n","from accelerate import Accelerator\n","from accelerate.logging import get_logger\n","from accelerate.utils import set_seed\n","\n","import transformers\n","from transformers import AutoTokenizer, PretrainedConfig, CLIPTextModel\n","\n","import diffusers\n","from diffusers import (\n","    AutoencoderKL,\n","    DDPMScheduler,\n","    StableDiffusionXLPipeline,\n","    UNet2DConditionModel,\n",")\n","from diffusers.loaders import LoraLoaderMixin\n","from diffusers.optimization import get_scheduler\n","from diffusers.utils import check_min_version\n","from diffusers.utils.import_utils import is_xformers_available\n","\n","import PIL\n","from PIL import Image\n","\n","logger = get_logger(__name__)"]},{"cell_type":"markdown","metadata":{"id":"nX5am_UN1fBw"},"source":["# Cell 3: Set up utility functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3Jm2kgi1fBw"},"outputs":[],"source":["def load_config(config_path):\n","    \"\"\"Load the configuration from a YAML file\"\"\"\n","    print(f\"üîç Loading configuration from {config_path}...\")\n","    with open(config_path, 'r') as f:\n","        config = yaml.safe_load(f)\n","    print(\"‚úÖ Configuration loaded successfully!\")\n","    return config\n","\n","def setup_logging(logging_dir):\n","    \"\"\"Set up logging configuration\"\"\"\n","    os.makedirs(logging_dir, exist_ok=True)\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","        handlers=[\n","            logging.StreamHandler(),\n","            logging.FileHandler(os.path.join(logging_dir, \"training.log\"))\n","        ]\n","    )\n","    logger.info(\"Logging setup complete\")"]},{"cell_type":"markdown","metadata":{"id":"jRc2L-mQ1fBw"},"source":["# Cell 4: Define the dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXgZX5ci1fBx"},"outputs":[],"source":["class MarketMindDataset(Dataset):\n","    \"\"\"Dataset for SDXL LoRA fine-tuning with MarketMind images and captions\"\"\"\n","\n","    def __init__(\n","        self,\n","        img_folder,\n","        caption_folder,\n","        tokenizer,\n","        tokenizer_2=None,\n","        width=1024,\n","        height=1024,\n","        center_crop=False,\n","        caption_ext=\"txt\",\n","        trigger_word=\"marketmind\"\n","    ):\n","        self.img_folder = Path(img_folder)\n","        self.caption_folder = Path(caption_folder)\n","        self.tokenizer = tokenizer\n","        self.tokenizer_2 = tokenizer_2 if tokenizer_2 is not None else tokenizer\n","        self.width = width\n","        self.height = height\n","        self.center_crop = center_crop\n","        self.caption_ext = caption_ext\n","        self.trigger_word = trigger_word\n","\n","        self.image_paths = list(self.img_folder.glob(\"*.jpg\")) + list(self.img_folder.glob(\"*.png\"))\n","        print(f\"üìä Found {len(self.image_paths)} images in dataset\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        img_name = img_path.stem\n","\n","        # Load and process image\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        # Resize and potentially crop the image\n","        if self.center_crop:\n","            image = self._center_crop_image(image)\n","        image = image.resize((self.width, self.height), resample=PIL.Image.LANCZOS)\n","\n","        # Convert to numpy and normalize to [0, 1]\n","        image_array = np.array(image) / 255.0\n","        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()\n","\n","        # Load caption\n","        caption_path = self.caption_folder / f\"{img_name}.{self.caption_ext}\"\n","        if caption_path.exists():\n","            with open(caption_path, 'r', encoding='utf-8') as f:\n","                caption = f.read().strip()\n","        else:\n","            print(f\"‚ö†Ô∏è No caption found for {img_name}, using default caption\")\n","            caption = f\"[{self.trigger_word}] an advertisement\"\n","\n","        # Make sure the caption has the trigger word\n","        if f\"[{self.trigger_word}]\" not in caption:\n","            caption = f\"[{self.trigger_word}] {caption}\"\n","\n","        # Encode caption for UNet conditioning (SDXL uses two text encoders)\n","        # First text encoder (CLIP ViT-L)\n","        tokenizer_output = self.tokenizer(\n","            caption,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=77,\n","            return_tensors=\"pt\"\n","        )\n","        prompt_embeds_input_ids = tokenizer_output.input_ids[0]\n","        prompt_embeds_attention_mask = tokenizer_output.attention_mask[0]\n","\n","        # Second text encoder (CLIP ViT-G)\n","        tokenizer_2_output = self.tokenizer_2(\n","            caption,\n","            padding=\"max_length\",\n","            truncation=True,\n","            max_length=77,\n","            return_tensors=\"pt\"\n","        )\n","        pooled_prompt_embeds_input_ids = tokenizer_2_output.input_ids[0]\n","        pooled_prompt_embeds_attention_mask = tokenizer_2_output.attention_mask[0]\n","\n","        return {\n","            \"pixel_values\": image_tensor,\n","            \"prompt_embeds_input_ids\": prompt_embeds_input_ids,\n","            \"prompt_embeds_attention_mask\": prompt_embeds_attention_mask,\n","            \"pooled_prompt_embeds_input_ids\": pooled_prompt_embeds_input_ids,\n","            \"pooled_prompt_embeds_attention_mask\": pooled_prompt_embeds_attention_mask,\n","            \"caption\": caption\n","        }\n","\n","    def _center_crop_image(self, image):\n","        \"\"\"Center crop the image to achieve a square aspect ratio\"\"\"\n","        width, height = image.size\n","        min_dim = min(width, height)\n","        left = (width - min_dim) // 2\n","        top = (height - min_dim) // 2\n","        right = left + min_dim\n","        bottom = top + min_dim\n","        return image.crop((left, top, right, bottom))"]},{"cell_type":"markdown","metadata":{"id":"Zklt1TbR1fBx"},"source":["# Cell 5: LoRA configuration and implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpBszKQe1fBx"},"outputs":[],"source":["def create_lora_layers(unet, text_encoder=None, text_encoder_2=None, rank=4, alpha=8):\n","    \"\"\"Apply LoRA adapters to the model components\"\"\"\n","    from peft import LoraConfig, get_peft_model\n","\n","    # Configure LoRA for UNet\n","    unet_target_modules = [\n","        \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",\n","        \"proj_in\", \"proj_out\",\n","        \"ff.net.0.proj\", \"ff.net.2\"\n","    ]\n","\n","    unet_config = LoraConfig(\n","        r=rank,\n","        lora_alpha=alpha,\n","        target_modules=unet_target_modules,\n","        lora_dropout=0.0,\n","        bias=\"none\"\n","    )\n","\n","    # Apply LoRA to UNet\n","    unet = get_peft_model(unet, unet_config)\n","\n","    # Apply LoRA to text encoders if requested\n","    if text_encoder is not None:\n","        text_encoder_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n","        text_encoder_config = LoraConfig(\n","            r=rank,\n","            lora_alpha=alpha,\n","            target_modules=text_encoder_target_modules,\n","            lora_dropout=0.0,\n","            bias=\"none\"\n","        )\n","        text_encoder = get_peft_model(text_encoder, text_encoder_config)\n","\n","    if text_encoder_2 is not None:\n","        text_encoder_2_target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n","        text_encoder_2_config = LoraConfig(\n","            r=rank,\n","            lora_alpha=alpha,\n","            target_modules=text_encoder_2_target_modules,\n","            lora_dropout=0.0,\n","            bias=\"none\"\n","        )\n","        text_encoder_2 = get_peft_model(text_encoder_2, text_encoder_2_config)\n","\n","    return unet, text_encoder, text_encoder_2"]},{"cell_type":"markdown","metadata":{"id":"6zNSzmNI1fBy"},"source":["# Cell 6: Training function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BMgbGcq1fBy"},"outputs":[],"source":["def train_model(config):\n","    \"\"\"Main training function for SDXL LoRA fine-tuning\"\"\"\n","\n","    # Set up accelerator\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=config[\"train\"][\"gradient_accumulation_steps\"],\n","        mixed_precision=\"fp16\" if config[\"dtype\"] == \"fp16\" else \"no\"\n","    )\n","\n","    # Make one log on every process with the configuration for debugging\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","    logger.info(accelerator.state, main_process_only=False)\n","\n","    # Set random seed for reproducibility\n","    set_seed(config[\"train\"].get(\"seed\", 42))\n","\n","    # Create output directories\n","    output_dir = Path(config[\"config\"][\"training_folder\"])\n","    output_dir.mkdir(exist_ok=True, parents=True)\n","\n","    # Load SDXL model components\n","    print(f\"üîÑ Loading model components from {config['model']['name_or_path']}...\")\n","\n","    # Load VAE\n","    vae = AutoencoderKL.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"vae\",\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    # Load text encoders\n","    text_encoder = CLIPTextModel.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"text_encoder\",\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    text_encoder_2 = CLIPTextModel.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"text_encoder_2\",\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    # Load tokenizers\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"tokenizer\",\n","        use_fast=False,\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    tokenizer_2 = AutoTokenizer.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"tokenizer_2\",\n","        use_fast=False,\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    # Load UNet\n","    unet = UNet2DConditionModel.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"unet\",\n","        revision=config[\"model\"].get(\"revision\", None)\n","    )\n","\n","    # Enable xFormers if available\n","    if is_xformers_available():\n","        import xformers\n","        unet.enable_xformers_memory_efficient_attention()\n","        text_encoder.enable_xformers_memory_efficient_attention()\n","        text_encoder_2.enable_xformers_memory_efficient_attention()\n","        print(\"‚úÖ xFormers enabled for memory-efficient attention\")\n","\n","    # Freeze VAE and text encoders\n","    vae.requires_grad_(False)\n","\n","    # Apply LoRA to UNet and optionally text encoders\n","    train_text_encoder = config[\"train\"].get(\"train_text_encoder\", False)\n","    train_text_encoder_2 = config[\"train\"].get(\"train_text_encoder_2\", False)\n","\n","    if not train_text_encoder:\n","        text_encoder.requires_grad_(False)\n","\n","    if not train_text_encoder_2:\n","        text_encoder_2.requires_grad_(False)\n","\n","    # Create LoRA layers\n","    print(\"üîÑ Applying LoRA adapters...\")\n","    rank = config[\"config\"][\"network\"][\"linear\"]\n","    alpha = config[\"config\"][\"network\"][\"linear_alpha\"]\n","\n","    unet, text_encoder_lora, text_encoder_2_lora = create_lora_layers(\n","        unet,\n","        text_encoder if train_text_encoder else None,\n","        text_encoder_2 if train_text_encoder_2 else None,\n","        rank=rank,\n","        alpha=alpha\n","    )\n","\n","    if train_text_encoder:\n","        text_encoder = text_encoder_lora\n","\n","    if train_text_encoder_2:\n","        text_encoder_2 = text_encoder_2_lora\n","\n","    # Create noise scheduler\n","    noise_scheduler = DDPMScheduler.from_pretrained(\n","        config[\"model\"][\"name_or_path\"],\n","        subfolder=\"scheduler\"\n","    )\n","\n","    # Create dataset and dataloader\n","    print(\"üîÑ Preparing dataset...\")\n","    dataset = MarketMindDataset(\n","        img_folder=config[\"datasets\"][0][\"folder_path\"],\n","        caption_folder=config[\"datasets\"][0][\"caption_folder\"],\n","        tokenizer=tokenizer,\n","        tokenizer_2=tokenizer_2,\n","        width=config[\"datasets\"][0].get(\"width\", 1024),\n","        height=config[\"datasets\"][0].get(\"height\", 1024),\n","        caption_ext=config[\"datasets\"][0].get(\"caption_ext\", \"txt\"),\n","        trigger_word=config[\"config\"][\"trigger_word\"]\n","    )\n","\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=config[\"train\"][\"batch_size\"],\n","        shuffle=True,\n","        num_workers=config[\"train\"].get(\"num_workers\", 2)\n","    )\n","\n","    # Prepare optimizer\n","    print(\"üîÑ Setting up optimizer and scheduler...\")\n","    trainable_params = []\n","    trainable_names = []\n","\n","    # Add UNet parameters\n","    for name, param in unet.named_parameters():\n","        if param.requires_grad:\n","            trainable_params.append(param)\n","            trainable_names.append(f\"unet.{name}\")\n","\n","    # Add text encoder parameters if needed\n","    if train_text_encoder:\n","        for name, param in text_encoder.named_parameters():\n","            if param.requires_grad:\n","                trainable_params.append(param)\n","                trainable_names.append(f\"text_encoder.{name}\")\n","\n","    if train_text_encoder_2:\n","        for name, param in text_encoder_2.named_parameters():\n","            if param.requires_grad:\n","                trainable_params.append(param)\n","                trainable_names.append(f\"text_encoder_2.{name}\")\n","\n","    # Print number of trainable parameters\n","    trainable_params_count = sum(p.numel() for p in trainable_params)\n","    print(f\"üî¢ Number of trainable parameters: {trainable_params_count:,}\")\n","\n","    # Create optimizer\n","    optimizer_class = torch.optim.AdamW\n","    optimizer = optimizer_class(\n","        trainable_params,\n","        lr=float(config[\"train\"][\"lr\"]),\n","        betas=(0.9, 0.999),\n","        weight_decay=config[\"train\"].get(\"weight_decay\", 1e-2),\n","        eps=config[\"train\"].get(\"adam_epsilon\", 1e-8)\n","    )\n","\n","    # Prepare for training with accelerator\n","    unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n","\n","    if train_text_encoder:\n","        text_encoder = accelerator.prepare(text_encoder)\n","\n","    if train_text_encoder_2:\n","        text_encoder_2 = accelerator.prepare(text_encoder_2)\n","\n","    # Move VAE and text encoders to device\n","    vae = vae.to(accelerator.device)\n","    if not train_text_encoder:\n","        text_encoder = text_encoder.to(accelerator.device)\n","    if not train_text_encoder_2:\n","        text_encoder_2 = text_encoder_2.to(accelerator.device)\n","\n","    # Create learning rate scheduler\n","    lr_scheduler = get_scheduler(\n","        config[\"train\"].get(\"lr_scheduler\", \"cosine\"),\n","        optimizer=optimizer,\n","        num_warmup_steps=int(config[\"train\"].get(\"warmup_steps\", 0)),\n","        num_training_steps=config[\"train\"][\"steps\"]\n","    )\n","\n","    # Prepare lr_scheduler with accelerator\n","    lr_scheduler = accelerator.prepare(lr_scheduler)\n","\n","    # Track global progress\n","    global_step = 0\n","    progress_bar = transformers.tqdm(\n","        range(config[\"train\"][\"steps\"]),\n","        disable=not accelerator.is_local_main_process\n","    )\n","    progress_bar.set_description(\"Training steps\")\n","\n","    # Training loop\n","    print(\"üöÄ Starting training loop...\")\n","    unet.train()\n","    if train_text_encoder:\n","        text_encoder.train()\n","    if train_text_encoder_2:\n","        text_encoder_2.train()\n","\n","    # Main training loop\n","    while global_step \u003c config[\"train\"][\"steps\"]:\n","        for batch in dataloader:\n","            # Skip if we've reached max steps\n","            if global_step \u003e= config[\"train\"][\"steps\"]:\n","                break\n","\n","            with accelerator.accumulate(unet):\n","                # Get input tensors\n","                pixel_values = batch[\"pixel_values\"].to(accelerator.device)\n","\n","                # Get text embeddings for conditioning\n","                with torch.no_grad():\n","                    if train_text_encoder:\n","                        prompt_embeds = text_encoder(\n","                            input_ids=batch[\"prompt_embeds_input_ids\"].to(accelerator.device),\n","                            attention_mask=batch[\"prompt_embeds_attention_mask\"].to(accelerator.device)\n","                        )[0]\n","                    else:\n","                        prompt_embeds = text_encoder(\n","                            input_ids=batch[\"prompt_embeds_input_ids\"].to(accelerator.device),\n","                            attention_mask=batch[\"prompt_embeds_attention_mask\"].to(accelerator.device)\n","                        )[0]\n","\n","                    if train_text_encoder_2:\n","                        pooled_prompt_embeds = text_encoder_2(\n","                            input_ids=batch[\"pooled_prompt_embeds_input_ids\"].to(accelerator.device),\n","                            attention_mask=batch[\"pooled_prompt_embeds_attention_mask\"].to(accelerator.device)\n","                        )[0]\n","                    else:\n","                        pooled_prompt_embeds = text_encoder_2(\n","                            input_ids=batch[\"pooled_prompt_embeds_input_ids\"].to(accelerator.device),\n","                            attention_mask=batch[\"pooled_prompt_embeds_attention_mask\"].to(accelerator.device)\n","                        )[0]\n","\n","                # Convert images to latent space\n","                with torch.no_grad():\n","                    latents = vae.encode(pixel_values).latent_dist.sample()\n","                    latents = latents * vae.config.scaling_factor\n","\n","                # Add noise to latents\n","                noise = torch.randn_like(latents)\n","                bsz = latents.shape[0]\n","                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n","                latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Predict the noise residual\n","                added_cond_kwargs = {\"text_embeds\": pooled_prompt_embeds, \"time_ids\": torch.zeros(bsz, 2).to(accelerator.device)}\n","\n","                model_pred = unet(\n","                    latents,\n","                    timesteps,\n","                    encoder_hidden_states=prompt_embeds,\n","                    added_cond_kwargs=added_cond_kwargs\n","                ).sample\n","\n","                # Get the target for loss calculation\n","                target = noise\n","\n","                # Calculate loss\n","                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n","\n","                # Backward pass and optimizer step\n","                accelerator.backward(loss)\n","\n","                if accelerator.sync_gradients:\n","                    params_to_clip = trainable_params\n","                    accelerator.clip_grad_norm_(params_to_clip, config[\"train\"].get(\"max_grad_norm\", 1.0))\n","\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad(set_to_none=True)\n","\n","            # Log progress\n","            if global_step % config[\"config\"].get(\"performance_log_every\", 10) == 0:\n","                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n","                progress_bar.set_postfix(**logs)\n","                accelerator.log(logs, step=global_step)\n","\n","            # Save checkpoint\n","            if global_step % config[\"save\"].get(\"save_every\", 500) == 0 and global_step \u003e 0:\n","                save_checkpoint(\n","                    accelerator, unet, text_encoder if train_text_encoder else None,\n","                    text_encoder_2 if train_text_encoder_2 else None,\n","                    tokenizer, tokenizer_2, global_step, output_dir, config\n","                )\n","\n","            # Generate sample images\n","            if global_step % config[\"sample\"].get(\"sample_every\", 500) == 0:\n","                generate_samples(\n","                    accelerator, unet, vae, text_encoder, text_encoder_2,\n","                    tokenizer, tokenizer_2, global_step, output_dir, config\n","                )\n","\n","            progress_bar.update(1)\n","            global_step += 1\n","\n","    # Save the final model\n","    save_checkpoint(\n","        accelerator, unet, text_encoder if train_text_encoder else None,\n","        text_encoder_2 if train_text_encoder_2 else None,\n","        tokenizer, tokenizer_2, global_step, output_dir, config, is_final=True\n","    )\n","\n","    print(\"‚úÖ Training complete!\")"]},{"cell_type":"markdown","metadata":{"id":"iUcOfuaJ1fBy"},"source":["# Cell 7: Function to save checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IwQCh5B1fBz"},"outputs":[],"source":["def save_checkpoint(accelerator, unet, text_encoder, text_encoder_2, tokenizer, tokenizer_2,\n","                   global_step, output_dir, config, is_final=False):\n","    \"\"\"Save a training checkpoint or the final model\"\"\"\n","\n","    # Wait for all processes to sync\n","    accelerator.wait_for_everyone()\n","\n","    # Determine save directory\n","    if is_final:\n","        save_dir = output_dir / f\"{config['config']['name']}_final\"\n","    else:\n","        save_dir = output_dir / f\"checkpoint-{global_step}\"\n","\n","    # Create directory\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    # Get unwrapped models\n","    unet_lora = accelerator.unwrap_model(unet)\n","\n","    # Save LoRA weights for UNet\n","    unet_lora_state_dict = get_peft_model_state_dict(unet_lora)\n","\n","    # Save state dicts\n","    if accelerator.is_main_process:\n","        print(f\"üíæ Saving checkpoint to {save_dir}\")\n","\n","        # Save UNet LoRA weights\n","        torch.save(unet_lora_state_dict, save_dir / \"unet_lora_state_dict.safetensors\")\n","\n","        # Save text encoder LoRA weights if trained\n","        if text_encoder is not None:\n","            text_encoder_lora = accelerator.unwrap_model(text_encoder)\n","            text_encoder_lora_state_dict = get_peft_model_state_dict(text_encoder_lora)\n","            torch.save(text_encoder_lora_state_dict, save_dir / \"text_encoder_lora_state_dict.safetensors\")\n","\n","        if text_encoder_2 is not None:\n","            text_encoder_2_lora = accelerator.unwrap_model(text_encoder_2)\n","            text_encoder_2_lora_state_dict = get_peft_model_state_dict(text_encoder_2_lora)\n","            torch.save(text_encoder_2_lora_state_dict, save_dir / \"text_encoder_2_lora_state_dict.safetensors\")\n","\n","        # Save tokenizers\n","        tokenizer.save_pretrained(save_dir / \"tokenizer\")\n","        tokenizer_2.save_pretrained(save_dir / \"tokenizer_2\")\n","\n","        # Save configuration\n","        with open(save_dir / \"config.yaml\", \"w\") as f:\n","            yaml.dump(config, f)\n","\n","    # Cleanup old checkpoints if needed and not final save\n","    if not is_final and accelerator.is_main_process:\n","        checkpoints = sorted(\n","            [d for d in output_dir.glob(\"checkpoint-*\") if d.is_dir()],\n","            key=lambda d: int(d.name.split(\"-\")[1])\n","        )\n","\n","        # Keep only the specified number of checkpoints\n","        max_checkpoints = config[\"save\"].get(\"max_step_saves_to_keep\", 5)\n","        if len(checkpoints) \u003e max_checkpoints:\n","            for old_checkpoint in checkpoints[:-max_checkpoints]:\n","                import shutil\n","                shutil.rmtree(old_checkpoint)\n","                print(f\"üóëÔ∏è Removed old checkpoint: {old_checkpoint}\")"]},{"cell_type":"markdown","metadata":{"id":"sfS9OA0d1fBz"},"source":["# Cell 8: Helper function to get PEFT model state dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNGj1rQS1fBz"},"outputs":[],"source":["def get_peft_model_state_dict(model):\n","    \"\"\"Extract the LoRA state dictionary from a PEFT model\"\"\"\n","    state_dict = {}\n","\n","    # Iterate through named parameters\n","    for name, param in model.named_parameters():\n","        if \"lora\" in name:\n","            state_dict[name] = param.data.cpu().clone()\n","\n","    return state_dict"]},{"cell_type":"markdown","metadata":{"id":"kQ6gbhAE1fBz"},"source":["# Cell 9: Function to generate samples during training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2I6SJAK1fBz"},"outputs":[],"source":["def generate_samples(accelerator, unet, vae, text_encoder, text_encoder_2,\n","                     tokenizer, tokenizer_2, global_step, output_dir, config):\n","    \"\"\"Generate sample images during training\"\"\"\n","\n","    if accelerator.is_main_process:\n","        print(f\"üñºÔ∏è Generating samples at step {global_step}\")\n","\n","        # Create pipeline for inference\n","        pipeline = StableDiffusionXLPipeline.from_pretrained(\n","            config[\"model\"][\"name_or_path\"],\n","            unet=accelerator.unwrap_model(unet),\n","            text_encoder=accelerator.unwrap_model(text_encoder),\n","            text_encoder_2=accelerator.unwrap_model(text_encoder_2),\n","            vae=vae,\n","            torch_dtype=torch.float16 if config[\"dtype\"] == \"fp16\" else torch.float32,\n","            revision=config[\"model\"].get(\"revision\", None)\n","        )\n","\n","        # Enable memory-efficient attention\n","        if is_xformers_available():\n","            pipeline.enable_xformers_memory_efficient_attention()\n","\n","        # Move to accelerator device\n","        pipeline = pipeline.to(accelerator.device)\n","\n","        # Set to eval mode for inference\n","        pipeline.unet.eval()\n","        pipeline.text_encoder.eval()\n","        pipeline.text_encoder_2.eval()\n","\n","        # Create samples directory\n","        samples_dir = output_dir / \"samples\"\n","        os.makedirs(samples_dir, exist_ok=True)\n","\n","        # Generate images for each prompt\n","        for i, prompt in enumerate(config[\"sample\"][\"prompts\"]):\n","            # Set seed for reproducibility\n","            base_seed = config[\"sample\"].get(\"seed\", 42)\n","\n","            if config[\"sample\"].get(\"walk_seed\", True):\n","                seed = base_seed + global_step // config[\"sample\"].get(\"sample_every\", 500) + i\n","            else:\n","                seed = base_seed + i\n","\n","            generator = torch.Generator(device=accelerator.device).manual_seed(seed)\n","\n","            # Generate image\n","            image = pipeline(\n","                prompt=prompt,\n","                negative_prompt=config[\"sample\"].get(\"neg\", \"\"),\n","                generator=generator,\n","                num_inference_steps=config[\"sample\"].get(\"sample_steps\", 30),\n","                guidance_scale=config[\"sample\"].get(\"guidance_scale\", 7.5)\n","            ).images[0]\n","\n","            # Save image\n","            sample_path = samples_dir / f\"step_{global_step:06d}_prompt_{i:02d}.png\"\n","            image.save(sample_path)\n","            print(f\"  üìÑ Saved sample to {sample_path}\")\n","\n","        # Clean up memory\n","        del pipeline\n","        torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"WYET7l2M1fBz"},"source":["# Cell 10: Main execution code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5k5Cc5ZC1fBz"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    print(\"üöÄ Starting SDXL LoRA fine-tuning for MarketMind\")\n","\n","    # Load configuration\n","    config_path = \"sdxl_lora_config.yaml\"\n","    config = load_config(config_path)\n","\n","    # Set up logging\n","    setup_logging(Path(config[\"config\"][\"training_folder\"]) / \"logs\")\n","\n","    # Start training\n","    train_model(config)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}