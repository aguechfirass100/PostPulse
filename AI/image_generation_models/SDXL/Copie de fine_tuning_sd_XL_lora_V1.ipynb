{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409373,
     "status": "ok",
     "timestamp": 1745776830917,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "09CXl0tLX9Ro",
    "outputId": "fa2fe37e-f679-41e4-d2e5-e80071069bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied to /content/drive/MyDrive/AI/processed_data2\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define paths for the original folders\n",
    "# images_folder = '/content/drive/MyDrive/AI/processed_data/all_images'  # Replace with your images folder path\n",
    "# captions_folder = '/content/drive/MyDrive/AI/processed_data/captions'  # Replace with your captions folder path\n",
    "# output_folder = '/content/drive/MyDrive/AI/processed_data2'  # Replace with your desired output folder path\n",
    "\n",
    "# # Ensure output folder exists\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "\n",
    "# # Get a list of all images and captions (assuming the names match)\n",
    "# image_files = os.listdir(images_folder)\n",
    "# caption_files = os.listdir(captions_folder)\n",
    "\n",
    "# # Make sure the image and caption files have matching names\n",
    "# for image_file in image_files:\n",
    "#     image_path = os.path.join(images_folder, image_file)\n",
    "#     caption_path = os.path.join(captions_folder, image_file.replace('.jpg', '.txt'))  # Assuming .jpg images and .txt captions\n",
    "\n",
    "#     if os.path.exists(caption_path):\n",
    "#         # Copy image and caption to the output folder\n",
    "#         shutil.copy(image_path, output_folder)\n",
    "#         shutil.copy(caption_path, output_folder)\n",
    "\n",
    "# print(f\"Files copied to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20795,
     "status": "ok",
     "timestamp": 1745883049550,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "7F96pQ9af_Xu",
    "outputId": "62ffcc93-be30-4130-cd7f-82e875bcc9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19679,
     "status": "ok",
     "timestamp": 1745883273334,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "f_uWhs7pgdSh",
    "outputId": "32b3dbdf-8969-425e-885b-3db339ee0c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ai-toolkit' already exists and is not an empty directory.\n",
      "/content/ai-toolkit\n",
      "Submodule path 'repositories/batch_annotator': checked out '420e142f6ad3cc14b3ea0500affc2c6c7e7544bf'\n",
      "Submodule path 'repositories/batch_annotator/repositories/controlnet': checked out 'e2b44154b72965c5e11b1ccee941d550682e4701'\n",
      "Submodule path 'repositories/ipadapter': checked out '5a18b1f3660acaf8bee8250692d6fb3548a19b14'\n",
      "Submodule path 'repositories/leco': checked out '9294adf40218e917df4516737afb13f069a6789d'\n",
      "Submodule path 'repositories/sd-scripts': checked out 'b78c0e2a69e52ce6c79abc6c8c82d1a9cabcf05c'\n",
      "Collecting git+https://github.com/huggingface/diffusers@363d1ab7e24c5ed6c190abb00df66d9edb74383b (from -r requirements.txt (line 5))\n",
      "  Cloning https://github.com/huggingface/diffusers (to revision 363d1ab7e24c5ed6c190abb00df66d9edb74383b) to /tmp/pip-req-build-39xdxv0t\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-39xdxv0t\n",
      "  Running command git rev-parse -q --verify 'sha^363d1ab7e24c5ed6c190abb00df66d9edb74383b'\n",
      "  Running command git fetch -q https://github.com/huggingface/diffusers 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
      "  Running command git checkout -q 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
      "  Resolved https://github.com/huggingface/diffusers to commit 363d1ab7e24c5ed6c190abb00df66d9edb74383b\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision==0.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
      "Requirement already satisfied: torchao==0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: transformers==4.49.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.49.0)\n",
      "Requirement already satisfied: lycoris-lora==1.8.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.8.3)\n",
      "Requirement already satisfied: flatten_json in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.1.14)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: oyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2.18.0)\n",
      "Requirement already satisfied: kornia in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.8.0)\n",
      "Requirement already satisfied: invisible-watermark in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.8.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (1.6.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.10.2)\n",
      "Requirement already satisfied: albumentations==1.4.15 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.4.15)\n",
      "Requirement already satisfied: albucore==0.0.16 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.0.16)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (2.11.3)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (2.3.0)\n",
      "Requirement already satisfied: k-diffusion in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (0.1.1.post1)\n",
      "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.32.0)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (1.0.15)\n",
      "Requirement already satisfied: prodigyopt in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (1.1.2)\n",
      "Requirement already satisfied: controlnet_aux==0.0.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.0.7)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (1.1.0)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (0.45.5)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.1.9)\n",
      "Requirement already satisfied: lpips in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (0.1.4)\n",
      "Requirement already satisfied: pytorch_fid in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.3.0)\n",
      "Requirement already satisfied: optimum-quanto==0.2.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.2.4)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.2.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (0.30.2)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (0.15.2)\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (5.27.1)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (8.0.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (4.11.0.86)\n",
      "Requirement already satisfied: pytorch-wavelets==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (1.3.0)\n",
      "Requirement already satisfied: numpy==1.26.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (1.26.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0->-r requirements.txt (line 2)) (11.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (1.15.2)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (0.25.2)\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (0.2.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.15->-r requirements.txt (line 17)) (4.11.0.86)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from controlnet_aux==0.0.7->-r requirements.txt (line 25)) (8.6.1)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from optimum-quanto==0.2.4->-r requirements.txt (line 31)) (1.11.1.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pytorch-wavelets==1.3.0->-r requirements.txt (line 38)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (5.29.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (75.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 11)) (3.1.3)\n",
      "Requirement already satisfied: kornia_rs>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from kornia->-r requirements.txt (line 12)) (0.1.8)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from invisible-watermark->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 15)) (5.9.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 19)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 19)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->-r requirements.txt (line 19)) (0.4.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->-r requirements.txt (line 20)) (4.9.3)\n",
      "Requirement already satisfied: clean-fid in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.1.35)\n",
      "Requirement already satisfied: clip-anytorch in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (2.6.0)\n",
      "Requirement already satisfied: dctorch in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.1.2)\n",
      "Requirement already satisfied: jsonmerge in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (1.9.2)\n",
      "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.2.5)\n",
      "Requirement already satisfied: torchsde in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.2.6)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from k-diffusion->-r requirements.txt (line 21)) (0.19.10)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open_clip_torch->-r requirements.txt (line 22)) (6.3.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.9.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (1.9.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.28.1)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (3.10.16)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (2.2.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.11.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.15.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 35)) (0.34.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.1->gradio->-r requirements.txt (line 35)) (15.0.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->-r requirements.txt (line 36)) (1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 35)) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 35)) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 35)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 35)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r requirements.txt (line 35)) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 35)) (2025.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (2025.3.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.15->-r requirements.txt (line 17)) (0.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (13.9.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch->-r requirements.txt (line 22)) (0.2.13)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->controlnet_aux==0.0.7->-r requirements.txt (line 25)) (3.21.0)\n",
      "Requirement already satisfied: jsonschema>2.4.0 in /usr/local/lib/python3.11/dist-packages (from jsonmerge->k-diffusion->-r requirements.txt (line 21)) (4.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0->-r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.11/dist-packages (from torchsde->k-diffusion->-r requirements.txt (line 21)) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (4.3.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (2.27.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->k-diffusion->-r requirements.txt (line 21)) (1.3.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r requirements.txt (line 21)) (4.0.12)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>2.4.0->jsonmerge->k-diffusion->-r requirements.txt (line 21)) (0.24.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->k-diffusion->-r requirements.txt (line 21)) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 35)) (0.1.2)\n",
      "Collecting numpy==1.26.3\n",
      "  Using cached numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed numpy-1.26.3\n"
     ]
    }
   ],
   "source": [
    "# =================== #\n",
    "# 2. Install Packages #\n",
    "# =================== #\n",
    "# !rm -rf ai-toolkit\n",
    "!git clone -b fix/numpy-error https://github.com/jhj0517/ai-toolkit.git\n",
    "%cd ai-toolkit\n",
    "!git submodule update --init --recursive --force\n",
    "!pip install -r requirements.txt\n",
    "!pip install --force-reinstall --no-deps numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WswiUbOYqZeL"
   },
   "outputs": [],
   "source": [
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1745883290719,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "l1LJHsGMgwHu"
   },
   "outputs": [],
   "source": [
    "# ======================= #\n",
    "# 3. HF Token Setup #\n",
    "# ======================= #\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"\"  # Replace with your token\n",
    "with open('/content/ai-toolkit/.env', 'w') as f:\n",
    "    f.write(f\"HF_TOKEN={hf_token}\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1745883294783,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "_17_dupig0AG"
   },
   "outputs": [],
   "source": [
    "# ======================= #\n",
    "# 4. Training Config #\n",
    "# ======================= #\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Copy config from Drive to repo\n",
    "config_src = \"/content/drive/MyDrive/AI/RenamedByFiras/SDXL/train_lora_SD_XL_24gb_Version2.yaml\"\n",
    "# config_dest = \"/content/ai-toolkit/config/my_flux_train.yaml\"\n",
    "config_dest = \"/content/ai-toolkit/config/train_lora_SD_XL_24gb_V2.yaml\"\n",
    "shutil.copyfile(config_src, config_dest)\n",
    "\n",
    "# Enable caching in config\n",
    "with open(config_dest) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['config']['process'][0]['datasets'][0]['cache_latents_to_disk'] = True\n",
    "config['config']['process'][0]['train']['performance_log_every'] = 250  # Log every 50 steps\n",
    "\n",
    "with open(config_dest, 'w') as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4R9gC8g2OV"
   },
   "outputs": [],
   "source": [
    "# # Run this FIRST in a separate cell\n",
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # Download FLUX.1-schnell to Drive (persists between sessions)\n",
    "# snapshot_download(\n",
    "#     \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "#     cache_dir=\"/content/drive/MyDrive/PIDEV/fine_tuning_flux1_schnell/sd15_cache\",\n",
    "#     resume_download=True,\n",
    "#     max_workers=4\n",
    "# )\n",
    "\n",
    "# # Download the training adapter\n",
    "# snapshot_download(\n",
    "#     \"ostris/FLUX.1-schnell-training-adapter\",\n",
    "#     cache_dir=\"/content/drive/MyDrive/PIDEV/flux_cache\",\n",
    "#     resume_download=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrfbBm2Og4Fb"
   },
   "outputs": [],
   "source": [
    "# # ======================= #\n",
    "# # 5. Training Execution #\n",
    "# # ======================= #\n",
    "# %cd /content/ai-toolkit\n",
    "\n",
    "# # Start training with automatic resume\n",
    "# !python run.py config/my_flux_train.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-y1n2ivzot6"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# captions_path = \"/content/drive/MyDrive/PIDEV/fine_tuning_flux1_schnell/datasets/captions\"\n",
    "\n",
    "# for fname in os.listdir(captions_path):\n",
    "#     if fname.endswith(\".txt\"):\n",
    "#         fpath = os.path.join(captions_path, fname)\n",
    "#         try:\n",
    "#             with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 content = f.read().strip()\n",
    "#                 if not isinstance(content, str):\n",
    "#                     print(f\"‚ùå Not a string: {fname} -> {type(content)}\")\n",
    "#                 elif content.lower() in [\"false\", \"none\", \"\"]:\n",
    "#                     print(f\"‚ö†Ô∏è Suspicious content in {fname}: {content!r}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"üí• Error reading {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vO5PUndWzlEG"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# captions_path = \"/content/drive/MyDrive/PIDEV/fine_tuning_flux1_schnell/datasets/captions\"\n",
    "\n",
    "# for fname in os.listdir(captions_path):\n",
    "#     if fname.endswith(\".txt\"):\n",
    "#         fpath = os.path.join(captions_path, fname)\n",
    "#         try:\n",
    "#             with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 content = f.read()\n",
    "\n",
    "#             # Remove all escaped quotes\n",
    "#             cleaned_content = content.replace('\\\\\"', '')\n",
    "\n",
    "#             # Save the cleaned content back to the file\n",
    "#             with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "#                 f.write(cleaned_content)\n",
    "\n",
    "#             print(f\"‚úÖ Cleaned: {fname}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"üí• Error cleaning {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4166,
     "status": "ok",
     "timestamp": 1745883304438,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "bDhGYfBeT6sg",
    "outputId": "edc0c4ed-09c4-48ef-d02e-bb1f8204fa5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Downloading hf_xet-1.0.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1218049,
     "status": "ok",
     "timestamp": 1745884525400,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "2gwRoOQAg60W",
    "outputId": "1c7118a2-f827-43f0-8dfc-17e9fc9561ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 job\n",
      "2025-04-28 23:35:18.282629: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 23:35:18.300494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745883318.322270    5729 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745883318.329024    5729 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-28 23:35:18.351135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.11/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "{\n",
      "    \"datasets\": [\n",
      "        {\n",
      "            \"cache_latents_to_disk\": true,\n",
      "            \"caption_dropout_rate\": 0.05,\n",
      "            \"caption_ext\": \"txt\",\n",
      "            \"folder_path\": \"/content/drive/MyDrive/AI/processed_data2\",\n",
      "            \"resolution\": [\n",
      "                512\n",
      "            ],\n",
      "            \"shuffle_tokens\": false\n",
      "        }\n",
      "    ],\n",
      "    \"device\": \"cuda:0\",\n",
      "    \"model\": {\n",
      "        \"is_xl\": true,\n",
      "        \"name_or_path\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
      "        \"quantize\": true\n",
      "    },\n",
      "    \"network\": {\n",
      "        \"linear\": 32,\n",
      "        \"linear_alpha\": 16,\n",
      "        \"type\": \"lora\"\n",
      "    },\n",
      "    \"performance_log_every\": 250,\n",
      "    \"sample\": {\n",
      "        \"guidance_scale\": 5.0,\n",
      "        \"height\": 512,\n",
      "        \"neg\": \"\",\n",
      "        \"prompts\": [\n",
      "            \"[marketmind] a vibrant social media ad poster promoting a summer sale, featuring bold text and colorful graphics\",\n",
      "            \"[marketmind] a sleek, modern ad poster for a tech product launch, with minimalist design and dynamic lighting\",\n",
      "            \"[marketmind] an eye-catching social media poster for a new caf\\u00e9 opening, showcasing cozy vibes and appetizing food photography\",\n",
      "            \"[marketmind] an Instagram ad poster for a skincare brand, featuring a close-up of a glowing face with soft natural light, minimalist pastel background, elegant text overlay that says 'Glow Naturally \\u2728 New Skincare Essentials \\u2013 Link in Bio'\",\n",
      "            \"[marketmind] a trendy Instagram ad poster for a streetwear brand, showing a stylish young adult leaning against a graffiti wall, urban color tones (neon and dark contrast), bold, edgy text saying 'Own The Streets \\ud83d\\ude80 Limited Drop Available Now'\",\n",
      "            \"[marketmind] a cozy and inviting poster for a coffee shop's autumn special, showing a hand holding a steaming cup with falling leaves around, warm brown and orange tones, and charming handwritten text saying 'Pumpkin Spice Season \\u2013 Sip the Magic'\",\n",
      "            \"[marketmind] a colorful social media ad poster for a fashion brand's spring collection, featuring a model in a flowy dress among blooming flowers, pastel tones, and bold text that says 'Blossom Into Style \\u2013 Shop Now'\"\n",
      "        ],\n",
      "        \"sample_every\": 250,\n",
      "        \"sample_steps\": 30,\n",
      "        \"sampler\": \"ddim\",\n",
      "        \"seed\": 42,\n",
      "        \"walk_seed\": true,\n",
      "        \"width\": 512\n",
      "    },\n",
      "    \"save\": {\n",
      "        \"dtype\": \"float16\",\n",
      "        \"max_step_saves_to_keep\": 4,\n",
      "        \"push_to_hub\": false,\n",
      "        \"save_every\": 250\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"batch_size\": 1,\n",
      "        \"dtype\": \"bf16\",\n",
      "        \"ema_config\": {\n",
      "            \"ema_decay\": 0.99,\n",
      "            \"use_ema\": true\n",
      "        },\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"lr\": 0.0001,\n",
      "        \"noise_scheduler\": \"flowmatch\",\n",
      "        \"optimizer\": \"adamw8bit\",\n",
      "        \"performance_log_every\": 250,\n",
      "        \"steps\": 2000,\n",
      "        \"train_text_encoder\": true,\n",
      "        \"train_unet\": true\n",
      "    },\n",
      "    \"training_folder\": \"/content/output\",\n",
      "    \"trigger_word\": \"marketmind\",\n",
      "    \"type\": \"sd_trainer\"\n",
      "}\n",
      "Using EMA\n",
      "\n",
      "#############################################\n",
      "# Running job: my_first_SD_XL_lora_v1\n",
      "#############################################\n",
      "\n",
      "\n",
      "Running  1 process\n",
      "model_index.json: 100% 609/609 [00:00<00:00, 4.39MB/s]\n",
      "config.json: 100% 565/565 [00:00<00:00, 3.59MB/s]\n",
      "text_encoder/model.safetensors: 100% 492M/492M [00:01<00:00, 267MB/s]\n",
      "config.json: 100% 575/575 [00:00<00:00, 3.26MB/s]\n",
      "text_encoder_2/model.safetensors: 100% 2.78G/2.78G [00:15<00:00, 183MB/s]\n",
      "merges.txt: 100% 525k/525k [00:00<00:00, 3.51MB/s]\n",
      "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.91MB/s]\n",
      "tokenizer_config.json: 100% 737/737 [00:00<00:00, 5.99MB/s]\n",
      "vocab.json: 100% 1.06M/1.06M [00:00<00:00, 6.95MB/s]\n",
      "special_tokens_map.json: 100% 460/460 [00:00<00:00, 3.87MB/s]\n",
      "tokenizer_config.json: 100% 725/725 [00:00<00:00, 5.43MB/s]\n",
      "config.json: 100% 1.68k/1.68k [00:00<00:00, 11.5MB/s]\n",
      "unet/diffusion_pytorch_model.safetensors: 100% 10.3G/10.3G [00:58<00:00, 175MB/s]\n",
      "config.json: 100% 642/642 [00:00<00:00, 5.50MB/s]\n",
      "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 187MB/s]\n",
      "vae_1_0/diffusion_pytorch_model.safetens(‚Ä¶): 100% 335M/335M [00:02<00:00, 151MB/s]\n",
      "Loading pipeline components...: 100% 7/7 [00:04<00:00,  1.58it/s]\n",
      "create LoRA network. base dim (rank): 32, alpha: 16\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder: 88 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "Dataset: /content/drive/MyDrive/AI/processed_data2\n",
      "  -  Preprocessing image dimensions\n",
      "100% 1750/1750 [00:00<00:00, 61980.43it/s]\n",
      "  -  Found 1750 images\n",
      "Bucket sizes for /content/drive/MyDrive/AI/processed_data2:\n",
      "512x512: 899 files\n",
      "448x576: 276 files\n",
      "448x512: 45 files\n",
      "256x320: 3 files\n",
      "384x576: 306 files\n",
      "448x448: 18 files\n",
      "384x640: 107 files\n",
      "320x576: 6 files\n",
      "576x448: 8 files\n",
      "512x448: 14 files\n",
      "320x256: 4 files\n",
      "384x512: 7 files\n",
      "320x704: 11 files\n",
      "512x384: 1 files\n",
      "320x448: 9 files\n",
      "320x384: 1 files\n",
      "320x320: 4 files\n",
      "320x512: 3 files\n",
      "640x384: 6 files\n",
      "576x384: 7 files\n",
      "256x448: 1 files\n",
      "576x320: 1 files\n",
      "192x256: 1 files\n",
      "384x384: 2 files\n",
      "320x640: 1 files\n",
      "192x320: 1 files\n",
      "640x320: 2 files\n",
      "256x384: 2 files\n",
      "192x192: 4 files\n",
      "29 buckets made\n",
      "Caching latents for /content/drive/MyDrive/AI/processed_data2\n",
      " - Saving latents to disk\n",
      "Caching latents to disk: 100% 1750/1750 [00:00<00:00, 7068.05it/s]\n",
      "Generating baseline samples before training\n",
      "my_first_SD_XL_lora_v1:   0% 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "my_first_SD_XL_lora_v1:  12% 249/2000 [06:08<45:10,  1.55s/it, lr: 1.0e-04 loss: 9.944e-01]\n",
      "Generating Images:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Generating Images:  14% 1/7 [00:10<01:00, 10.10s/it]\u001b[A\n",
      "Generating Images:  29% 2/7 [00:20<00:50, 10.08s/it]\u001b[A\n",
      "Generating Images:  43% 3/7 [00:30<00:40, 10.05s/it]\u001b[A\n",
      "Generating Images:  57% 4/7 [00:40<00:30, 10.09s/it]\u001b[A\n",
      "Generating Images:  71% 5/7 [00:50<00:20, 10.08s/it]\u001b[A\n",
      "Generating Images:  86% 6/7 [01:00<00:10, 10.07s/it]\u001b[A\n",
      "Generating Images: 100% 7/7 [01:10<00:00, 10.07s/it]\u001b[A\n",
      "                                                    \u001b[ASaving at step 250\n",
      "Saved to /content/output/my_first_SD_XL_lora_v1/optimizer.pt\n",
      "\n",
      "Timer 'my_first_SD_XL_lora_v1 Timer':\n",
      " - 1.4589s avg - train_loop, num = 10\n",
      " - 0.7788s avg - backward, num = 10\n",
      " - 0.5049s avg - predict_unet, num = 10\n",
      " - 0.0737s avg - optimizer_step, num = 10\n",
      " - 0.0690s avg - encode_prompt, num = 10\n",
      " - 0.0081s avg - preprocess_batch, num = 10\n",
      " - 0.0070s avg - prepare_noise, num = 10\n",
      " - 0.0018s avg - get_batch, num = 10\n",
      " - 0.0004s avg - calculate_loss, num = 10\n",
      " - 0.0002s avg - prepare_latents, num = 10\n",
      " - 0.0001s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - log_to_tensorboard, num = 2\n",
      "\n",
      "my_first_SD_XL_lora_v1:  25% 499/2000 [12:13<36:55,  1.48s/it, lr: 1.0e-04 loss: 6.894e-01]\n",
      "Generating Images:   0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Generating Images:  14% 1/7 [00:10<01:00, 10.07s/it]\u001b[A\n",
      "Generating Images:  29% 2/7 [00:20<00:50, 10.00s/it]\u001b[A\n",
      "Generating Images:  43% 3/7 [00:30<00:40, 10.03s/it]\u001b[A\n",
      "Generating Images:  57% 4/7 [00:40<00:30, 10.04s/it]\u001b[A\n",
      "Generating Images:  71% 5/7 [00:50<00:20, 10.04s/it]\u001b[A\n",
      "Generating Images:  86% 6/7 [01:00<00:10, 10.02s/it]\u001b[A\n",
      "Generating Images: 100% 7/7 [01:10<00:00,  9.99s/it]\u001b[A\n",
      "                                                    \u001b[ASaving at step 500\n",
      "Saved to /content/output/my_first_SD_XL_lora_v1/optimizer.pt\n",
      "\n",
      "Timer 'my_first_SD_XL_lora_v1 Timer':\n",
      " - 1.4619s avg - train_loop, num = 10\n",
      " - 0.7905s avg - backward, num = 10\n",
      " - 0.4592s avg - predict_unet, num = 10\n",
      " - 0.0737s avg - optimizer_step, num = 10\n",
      " - 0.0666s avg - encode_prompt, num = 10\n",
      " - 0.0076s avg - preprocess_batch, num = 10\n",
      " - 0.0066s avg - prepare_noise, num = 10\n",
      " - 0.0017s avg - get_batch, num = 10\n",
      " - 0.0004s avg - calculate_loss, num = 10\n",
      " - 0.0002s avg - prepare_latents, num = 10\n",
      " - 0.0001s avg - batch_cleanup, num = 10\n",
      " - 0.0000s avg - scheduler_step, num = 10\n",
      " - 0.0000s avg - prepare_prompt, num = 10\n",
      " - 0.0000s avg - grad_setup, num = 10\n",
      " - 0.0000s avg - log_to_tensorboard, num = 3\n",
      "\n",
      "my_first_SD_XL_lora_v1:  29% 573/2000 [14:00<33:58,  1.43s/it, lr: 1.0e-04 loss: 5.849e-01]\n",
      "========================================\n",
      "Result:\n",
      " - 0 completed jobs\n",
      "========================================\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 493, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/bitsandbytes/optim/optimizer.py\", line 292, in step\n",
      "    torch.cuda.synchronize()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\", line 985, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/ai-toolkit/run.py\", line 119, in <module>\n",
      "    main()\n",
      "  File \"/content/ai-toolkit/run.py\", line 115, in main\n",
      "    raise e\n",
      "  File \"/content/ai-toolkit/run.py\", line 95, in main\n",
      "    job.run()\n",
      "  File \"/content/ai-toolkit/jobs/ExtensionJob.py\", line 22, in run\n",
      "    process.run()\n",
      "  File \"/content/ai-toolkit/jobs/process/BaseSDTrainProcess.py\", line 2010, in run\n",
      "    loss_dict = self.hook_train_loop(batch_list)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/ai-toolkit/extensions_built_in/sd_trainer/SDTrainer.py\", line 1779, in hook_train_loop\n",
      "    self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/optimizer.py\", line 178, in step\n",
      "    self.optimizer.step(closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\", line 140, in wrapper\n",
      "    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 478, in wrapper\n",
      "    with torch.autograd.profiler.record_function(profile_name):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\", line 769, in __exit__\n",
      "    torch.ops.profiler._record_function_exit._RecordFunction(record)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 947, in __call__\n",
      "    if _must_dispatch_in_python(args, kwargs):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1000, in _must_dispatch_in_python\n",
      "    def _must_dispatch_in_python(args, kwargs):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python run.py /content/ai-toolkit/config/train_lora_SD_XL_24gb_V2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1745881527220,
     "user": {
      "displayName": "Med Larg8",
      "userId": "03828150558657363322"
     },
     "user_tz": -60
    },
    "id": "kPeiK2lixRle",
    "outputId": "c25016c3-af79-4deb-9b14-9e65ff27e44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder copied successfully!\n"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Source folder path\n",
    "# source_folder = '/content/output/my_first_SD_XL_lora_v1/samples'\n",
    "\n",
    "# # Destination folder path\n",
    "# destination_folder = '/content/drive/MyDrive/AI/RenamedByFiras/SDXL/images/im'\n",
    "\n",
    "# # Copy the folder\n",
    "# shutil.copytree(source_folder, destination_folder)\n",
    "\n",
    "# print(\"Folder copied successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxwBb6xtywW2"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from diffusers import DiffusionPipeline\n",
    "# from PIL import Image\n",
    "\n",
    "# def generate_image_with_flux_lora(\n",
    "#     prompt: str,\n",
    "#     lora_path: str,\n",
    "#     checkpoint: str = \"latest\",\n",
    "#     output_path: str = \"flux_generated_image.png\",\n",
    "#     base_model: str = \"stabilityai/flux-1\",\n",
    "#     device: str = \"cuda\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Generate an image using your fine-tuned Flux.1 LoRA model.\n",
    "\n",
    "#     Args:\n",
    "#         prompt (str): Your text prompt for image generation\n",
    "#         lora_path (str): Path to your LoRA output directory\n",
    "#         checkpoint (str): Which checkpoint to use ('latest' or specific step like '000001500')\n",
    "#         output_path (str): Where to save the generated image\n",
    "#         base_model (str): Base Flux model (default: \"stabilityai/flux-1\")\n",
    "#         device (str): Device to use ('cuda' or 'cpu')\n",
    "#     \"\"\"\n",
    "#     # Load base Flux model\n",
    "#     pipe = DiffusionPipeline.from_pretrained(\n",
    "#         base_model,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         variant=\"fp16\"\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Determine which LoRA checkpoint to load\n",
    "#     if checkpoint == \"latest\":\n",
    "#         lora_file = f\"my_first_flux_lora_v1.safetensors\"\n",
    "#     else:\n",
    "#         lora_file = f\"my_first_flux_lora_v1_{checkpoint}.safetensors\"\n",
    "\n",
    "#     # Load your LoRA weights\n",
    "#     pipe.load_lora_weights(lora_path, weight_name=lora_file)\n",
    "\n",
    "#     # Generate image\n",
    "#     print(f\"Generating image with prompt: '{prompt}'...\")\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         num_inference_steps=50,\n",
    "#         guidance_scale=7.5\n",
    "#     ).images[0]\n",
    "\n",
    "#     # Save image\n",
    "#     image.save(output_path)\n",
    "#     print(f\"Image saved to {output_path}\")\n",
    "#     return image\n",
    "\n",
    "# # Example usage\n",
    "# generate_image_with_flux_lora(\n",
    "#     prompt=\"a futuristic cyberpunk cityscape at night with neon lights, Flux style\",\n",
    "#     lora_path=\"./output/my_first_flux_lora_v1\",\n",
    "#     checkpoint=\"000001500\",  # or \"latest\"\n",
    "#     output_path=\"flux_output.png\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
