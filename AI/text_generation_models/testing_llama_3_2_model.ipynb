{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
      ],
      "metadata": {
        "id": "9--NpO7N0CuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if you're using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ChLuCstq13RR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "d779cdfb-e7ce-4a2c-96a2-f562a4d1de07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7422aa535971>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive (if you're using Google Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoFgpXGAz605"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Define your parameters\n",
        "model_path = \"/content/drive/MyDrive/my_fine_tuned_llama_3\"\n",
        "max_seq_length = 512  # Adjust the max sequence length as per your model's configuration\n",
        "dtype = torch.float16  # Use this if you're targeting mixed precision for faster performance on GPUs\n",
        "load_in_4bit = False  # Set this to True if you want to load the model in 4-bit precision\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference (if supported)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the input messages for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a fun Instagram caption for a party at club. Use a playful tone, emojis, and hashtags and use gen z slang and talk about tha amazing dj and the caption needs to showcase the fear of missing out in the reader\"},\n",
        "]\n",
        "\n",
        "# Tokenize the input messages and prepare for generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Move to GPU if available\n",
        "\n",
        "# Initialize the TextStreamer to stream text generation\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the text from the model\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,  # Adjust the number of tokens as per your need\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable native 2x faster inference (if supported)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the input messages for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a fun Instagram caption for a new skin care line im launching,i want the caption to make my client fear to missing out on  and witty and gen z like\"},\n",
        "]\n",
        "\n",
        "# Tokenize the input messages and prepare for generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Move to GPU if available\n",
        "\n",
        "# Initialize the TextStreamer to stream text generation\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the text from the model\n",
        "_ = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,  # Adjust the number of tokens as per your need\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "01Eq8FK43Uyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert-score\n"
      ],
      "metadata": {
        "id": "HTxJ28fX2glY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TextStreamer\n",
        "from unsloth import FastLanguageModel\n",
        "from bert_score import score\n",
        "\n",
        "# Define your parameters\n",
        "model_path = \"/content/drive/MyDrive/my_fine_tuned_llama_3\"\n",
        "max_seq_length = 512  # Adjust the max sequence length as per your model's configuration\n",
        "dtype = torch.float16  # Use this if you're targeting mixed precision for faster performance on GPUs\n",
        "load_in_4bit = False  # Set this to True if you want to load the model in 4-bit precision\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference (if supported)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the input messages for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a fun Instagram caption for a party at club. Use a playful tone, emojis, and hashtags and use gen z slang and talk about tha amazing dj and the caption needs to showcase the fear of missing out in the reader\"},\n",
        "]\n",
        "\n",
        "# Tokenize the input messages and prepare for generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Move to GPU if available\n",
        "\n",
        "# Initialize the TextStreamer to stream text generation\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the text from the model\n",
        "generated_text = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,  # Adjust the number of tokens as per your need\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n",
        "\n",
        "# Convert the generated tokens to text\n",
        "generated_response = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(\"Generated Response:\")\n",
        "print(generated_response)\n",
        "\n",
        "# Evaluate using BERTScore (this compares the generated text with a reference text)\n",
        "# You should have a reference text (ground truth) for comparison\n",
        "reference_text = \"Write a fun Instagram caption for a party at club. Use a playful tone, emojis, and hashtags and use gen z slang and talk about tha amazing dj and the caption needs to showcase the fear of missing out in the reader\"\n",
        "\n",
        "# Compute BERTScore\n",
        "precision, recall, f1 = score([generated_response], [reference_text], lang='en')\n",
        "\n",
        "# Print BERTScore\n",
        "print(\"\\nBERTScore:\")\n",
        "print(f\"Precision: {precision.mean():.4f}\")\n",
        "print(f\"Recall: {recall.mean():.4f}\")\n",
        "print(f\"F1: {f1.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "H2KBg5lm2hfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score nltk\n"
      ],
      "metadata": {
        "id": "UHpEaBBH3R1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TextStreamer\n",
        "from unsloth import FastLanguageModel\n",
        "from bert_score import score as bert_score\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')  # Required for BLEU evaluation\n",
        "\n",
        "# Define your parameters\n",
        "model_path = \"/content/drive/MyDrive/my_fine_tuned_llama_3\"\n",
        "max_seq_length = 512  # Adjust the max sequence length as per your model's configuration\n",
        "dtype = torch.float16  # Use this if you're targeting mixed precision for faster performance on GPUs\n",
        "load_in_4bit = False  # Set this to True if you want to load the model in 4-bit precision\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "# Enable native 2x faster inference (if supported)\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the input messages for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a fun Instagram caption for a party at Luna Lounges. Use a playful tone, emojis, and hashtags like #LunaNights #PartyVibes and make it more gen z\"},\n",
        "]\n",
        "\n",
        "# Tokenize the input messages and prepare for generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Move to GPU if available\n",
        "\n",
        "# Initialize the TextStreamer to stream text generation\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the text from the model\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,  # Adjust the number of tokens as per your need\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n",
        "\n",
        "# Convert the generated tokens to text\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Response:\")\n",
        "print(generated_text)\n",
        "\n",
        "# Define your reference text (ground truth)\n",
        "reference_text = \"Had a blast at the club ðŸŽ§ðŸ”¥ Amazing DJ and wild vibes all night! #LunaNights #PartyVibes\"\n",
        "\n",
        "# === Automated Metrics ===\n",
        "\n",
        "# 1) BERTScore\n",
        "P, R, F1 = bert_score([generated_text], [reference_text], lang=\"en\", verbose=False)\n",
        "print(f\"\\nBERTScore â†’ Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1: {F1.item():.4f}\")\n",
        "\n",
        "# 2) ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference_text, generated_text)\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for name, score in rouge_scores.items():\n",
        "    print(f\"  {name}: P={score.precision:.4f}, R={score.recall:.4f}, F={score.fmeasure:.4f}\")\n",
        "\n",
        "# 3) BLEU\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu = sentence_bleu([reference_text.split()], generated_text.split(), smoothing_function=smooth)\n",
        "print(f\"\\nBLEU Score â†’ {bleu:.4f}\")\n"
      ],
      "metadata": {
        "id": "4xfbdqPs3SVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable native 2x faster inference (if supported)\n",
        "FastLanguageModel.for_inference(model)\n",
        "prompt='Write a fun Instagram caption for a party at Luna Lounges. Use a playful tone, emojis, and hashtags like #LunaNights #PartyVibes and make it more gen z'\n",
        "# Prepare the input messages for the model\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "# Tokenize the input messages and prepare for generation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Move to GPU if available\n",
        "\n",
        "# Initialize the TextStreamer to stream text generation\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the text from the model\n",
        "generated_ids = model.generate(\n",
        "    input_ids=inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,  # Adjust the number of tokens as per your need\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")\n",
        "\n",
        "# Convert the generated tokens to text\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Response:\")\n",
        "print(generated_text)\n",
        "\n",
        "# Define your reference text (ground truth)\n",
        "reference_text = \"Luna Lounges got the vibe you need tonight! ðŸ¥‚ðŸ’ƒ Dance floorâ€™s lit, drinks are flowing, and the good times are rolling. Whoâ€™s ready to turn up? ðŸ˜ŽðŸ”¥ #LunaNights #PartyVibes #TurnUpTheBeat #GoodVibesOnly #SquadGoals #VibingAndThriving ðŸª©ðŸŽ¶\"\n",
        "\n",
        "# === Automated Metrics ===\n",
        "\n",
        "# 1) BERTScore\n",
        "P, R, F1 = bert_score([generated_text], [reference_text], lang=\"en\", verbose=False)\n",
        "print(f\"\\nBERTScore â†’ Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1: {F1.item():.4f}\")\n",
        "\n",
        "# 2) ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference_text, generated_text)\n",
        "print(\"\\nROUGE Scores:\")\n",
        "for name, score in rouge_scores.items():\n",
        "    print(f\"  {name}: P={score.precision:.4f}, R={score.recall:.4f}, F={score.fmeasure:.4f}\")\n",
        "\n",
        "# 3) BLEU\n",
        "smooth = SmoothingFunction().method1\n",
        "bleu = sentence_bleu([reference_text.split()], generated_text.split(), smoothing_function=smooth)\n",
        "print(f\"\\nBLEU Score â†’ {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "lfZEFX4Q4ba3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# Tokenize the generated and reference texts\n",
        "generated_tokens = word_tokenize(generated_text)\n",
        "reference_tokens = word_tokenize(reference_text)\n",
        "\n",
        "# Calculate the METEOR score\n",
        "meteor = meteor_score([reference_tokens], generated_tokens)\n",
        "print(f\"METEOR Score: {meteor:.4f}\")\n"
      ],
      "metadata": {
        "id": "YWe6OaET7uci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}